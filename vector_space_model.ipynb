{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model \n",
    "### Cristian Curaba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependancy libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math as m\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining global variables for directories\n",
    "in_path_files=os.getcwd()+\"/cranfieldDocs\"\n",
    "out_path_files=os.getcwd()+\"/preprocessed_cranfieldDocs\"\n",
    "path=os.getcwd()\n",
    "\n",
    "if not os.path.isdir(out_path_files):\n",
    "    os.mkdir(out_path_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Steps:**\n",
    "\n",
    "- Transform the input text to lowercase.\n",
    "- Remove punctuation, numbers.\n",
    "- Exclude words with one or two characters in length.\n",
    "- Perform stemming.\n",
    "- Eliminate stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiallizing Porter Stemmer object\n",
    "st = PorterStemmer()\n",
    "\n",
    "#Intializing regular expression object to remove words with one or two characters length\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "\n",
    "# Intializing stopwords list\n",
    "stop_list = stopwords.words('english')\n",
    "\n",
    "\n",
    "def preprocess_text(input_text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text. Converts to lower case,\n",
    "    removes punctuation and numbers, splits on whitespaces, \n",
    "    removes stopwords, performs stemming & removes words with one or \n",
    "    two characters length.\n",
    "\n",
    "    Arguments:\n",
    "        input_text {string} -- text to be tokenized\n",
    "\n",
    "    Returns:\n",
    "        string -- string of tokens generated\n",
    "    \"\"\"\n",
    "\n",
    "    # Converting to lower case\n",
    "    text_lower = input_text.lower()\n",
    "\n",
    "    # Removing anythig that is not a word character or a whitespace\n",
    "    text_no_punct = re.sub(r'[^\\w\\s]', '', text_lower)\n",
    "\n",
    "    # Removing numbers\n",
    "    text_no_numbers = re.sub(r'[0-9]', '', text_no_punct)\n",
    "\n",
    "    # Removing tokens with one or two characters length\n",
    "    text_no_short_words = shortword.sub('', text_no_numbers)\n",
    "\n",
    "    # Splitting on whitespaces to generate tokens\n",
    "    tokens = text_no_short_words.split()\n",
    "\n",
    "    # Removing stop words from the tokens\n",
    "    clean_tokens = [word for word in tokens if word not in stop_list]\n",
    "\n",
    "    # Stemming the tokens\n",
    "    stem_tokens = [st.stem(word) for word in clean_tokens]\n",
    "\n",
    "    # Checking for stopwords again\n",
    "    clean_stem_tokens = [word for word in stem_tokens if word not in stop_list]\n",
    "\n",
    "    # Converting list of tokens to string\n",
    "    clean_stem_tokens_final = ' '.join(map(str,  clean_stem_tokens))\n",
    "    \n",
    "\n",
    "    return clean_stem_tokens_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to extract tokens from a file\n",
    "def extractTokens(beautifulSoup, tag):\n",
    "    \"\"\"Extract tokens of the text between a specific SGML <tag>.\n",
    "\n",
    "    Arguments:\n",
    "        beautifulSoup {bs4.BeautifulSoup} -- soup bs object formed using text of a file\n",
    "        tag {string} -- target SGML <tag>\n",
    "\n",
    "    Returns:\n",
    "        string -- string of tokens extracted from text between the target SGML <tag>\n",
    "    \"\"\"\n",
    "\n",
    "    # find the tag and get its text content\n",
    "    tag_content = beautifulSoup.find(tag).text if beautifulSoup.find(tag) else ''\n",
    "\n",
    "    return tag_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performed pre-processing phase to corpus and queries.\n",
    "\n",
    "Here I joined title and text of docs toghether, an improvement is possible: giving more relevance for title occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing files done!\n",
      "Preprocessing queries done!\n"
     ]
    }
   ],
   "source": [
    "filenames = os.listdir(in_path_files)\n",
    "\n",
    "for fname in filenames:\n",
    "\n",
    "    # generate filenames\n",
    "    infilepath = os.path.join(in_path_files, fname)\n",
    "    outfilepath = os.path.join(out_path_files, fname)\n",
    "\n",
    "\n",
    "    with open(infilepath) as infile:\n",
    "        with open(outfilepath, 'w') as outfile:\n",
    "\n",
    "            # read all text in a file\n",
    "            fileData = infile.read()\n",
    "\n",
    "            # creating BeautifulSoup object to extract text between SGML tags\n",
    "            soup = BeautifulSoup(fileData)\n",
    "\n",
    "            # extract tokens for <title>\n",
    "            title = extractTokens(soup, 'title')\n",
    "\n",
    "            # extract tokens for <text>\n",
    "            text = extractTokens(soup, 'text')\n",
    "\n",
    "            # preprocess tokens for <title>\n",
    "            title = preprocess_text(title)\n",
    "\n",
    "            # preprocess tokens for <text>\n",
    "            text = preprocess_text(text)\n",
    "            \n",
    "            # write tokens for <title> into new file\n",
    "            outfile.write(title)\n",
    "            outfile.write(\" \")\n",
    "\n",
    "            # write tokens for <text> into new file\n",
    "            outfile.write(text)\n",
    "        outfile.close()\n",
    "    infile.close()\n",
    "\n",
    "print(\"Preprocessing files done!\")\n",
    "\n",
    "#Preprocessing query file\n",
    "query_file=os.getcwd()+\"/queries.txt\"\n",
    "query_file_out=os.getcwd()+\"/preprocessed_queries.txt\"\n",
    "\n",
    "# Preprocessing the queries.txt file\n",
    "q = open(query_file, 'r')\n",
    "\n",
    "# opening new file to write preprocessed tokens into\n",
    "new_q = open(query_file_out, 'w')\n",
    "\n",
    "# read each line of file seperately\n",
    "text = q.readlines()\n",
    "for line in text:\n",
    "    \n",
    "    # if condition to avoid newline character in the end of file\n",
    "    if(line != text[-1]):\n",
    "        query_tokens = preprocess_text(line.rstrip())\n",
    "        new_q.write(query_tokens + '\\n')\n",
    "    else:\n",
    "        query_tokens = preprocess_text(line.rstrip())\n",
    "        new_q.write(query_tokens)\n",
    "\n",
    "q.close()\n",
    "new_q.close()\n",
    "\n",
    "print(\"Preprocessing queries done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single list of all preprocessed docs\n",
    "all_docs = []\n",
    "\n",
    "for fname in filenames:\n",
    "    outfilepath = out_path_files + '/' + fname\n",
    "    with open(outfilepath) as file:\n",
    "        fileData = file.read()\n",
    "        all_docs.append(fileData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tf-idf dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build $\\texttt{DF}$ dictionary with pairs $\\texttt{(token, number of occurences)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Calculating document frequency\n",
    "# Create a defaultdict with integers as default values\n",
    "DF = defaultdict(int)\n",
    "\n",
    "# Populate the dictionary with document occurrences for each token\n",
    "for doc_index, doc in enumerate(all_docs):\n",
    "    tokens = set(doc.split())\n",
    "    for token in tokens:\n",
    "        DF[token] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I calculate the $\\texttt{tf\\_idf}= \\texttt{tf}\\times \\texttt{idf}$ dictionary with all pairs $(\\texttt{token, tf\\_idf})$ where $$\\texttt{tf}=\\frac{\\texttt{freq}_{t,d}}{\\texttt{len}(d)}$$\n",
    " with $t$ token, $d$ document and $\\texttt{freq}_{t,d}$ is the number of occurence of token $t$ in the document $d$;\n",
    "$$\n",
    "idf_t= \\log \\frac{N}{df_t}, \n",
    "$$ \n",
    "with $t$ token, $N$ nummber of docs and $df_t$ the document frequency of the token $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_idf value of 'approach' in doc number 15:  0.048112262483488724\n",
      "tf_idf value of 'experi' in doc number 1:  0.00877713364724684\n",
      "tf_idf value of 'stress' in doc number 1400:  0.08963980961566191\n"
     ]
    }
   ],
   "source": [
    "# Calculating tf-idf values for each term in the corpus\n",
    "# tf-idf_{t,d} = tf_{t,d} * idf_{t,d} = tf_{t,d} * log(N/df_{t})\n",
    "\n",
    "# vocabulary of all the terms in the corpus\n",
    "vocab = [term for term in DF]\n",
    "# creating dictionary to store tf-idf values for each term in the vocabulary\n",
    "tf_idf = {}\n",
    "\n",
    "#doc is the index of the document in the corpus\n",
    "doc = 0\n",
    "\n",
    "for i in range(len(all_docs)):\n",
    "    # tokenizing each document\n",
    "    tokens = all_docs[i].split()\n",
    "    \n",
    "    # counter object to efficiently count number of occurence of a term in a particular document\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in tokens:\n",
    "        \n",
    "        # counting occurence of term in document using counter object\n",
    "        tf = counter[token]/words_count\n",
    "        \n",
    "        # retrieving df values from DF dictionary\n",
    "        df = DF[token] if token in vocab else 0\n",
    "        \n",
    "        # adding 1 to numerator & denominator to avoid divide by 0 error\n",
    "        idf = np.log((len(all_docs)+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc, token] = tf*idf\n",
    "    doc += 1\n",
    "    \n",
    "#printing some tf-idf values\n",
    "print(f'tf_idf value of \\'approach\\' in doc number 15: ', tf_idf[14, 'approach'])\n",
    "print(f'tf_idf value of \\'experi\\' in doc number 1: ', tf_idf[0, 'experiment'])\n",
    "print(f'tf_idf value of \\'stress\\' in doc number 1400: ', tf_idf[1399, 'stress'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplfy, we build a TD_IDF matrix with size (number of docs, number of tokes). We will get a sparse matrix with consume more memory than a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the tf-idf document-term matrix with numpy\n",
    "TF_IDF = np.zeros((len(all_docs), len(vocab)))\n",
    "\n",
    "# creating vector of tf-idf values\n",
    "for pair in tf_idf:\n",
    "    ind = vocab.index(pair[1])\n",
    "    TF_IDF[pair[0]][ind] = tf_idf[pair]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code snippet, we will calculate the cosine similarity between two vectors using the $\\texttt{cosine\\_similarity} $ function. This function takes two numpy arrays as input and returns the cosine similarity between them. We will also define the $\\texttt{ranking}$ function, which will determine a ranked list of top k documents based on their cosine similarity with a given query. Finally, we will use the $\\texttt{generate\\_ranked\\_document\\_list}$ function to generate a ranked list of documents in descending order of their cosine similarity with the queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to generate vector representation of tokens where v(d)\\in R^|V|, [v(d)]_i = tf_{t_i,d} * idf_{t_i}\n",
    "def generate_vector(tokens):\n",
    "    \"\"\"\n",
    "    Create a vector based on the vocabulary for the given tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of tokens to be converted.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Vector representation of tokens.\n",
    "    \"\"\"\n",
    "    vector = np.zeros(len(vocab))\n",
    "    token_counts = Counter(tokens)\n",
    "    total_words = len(tokens)\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        term_frequency = token_counts[token] / total_words\n",
    "        document_frequency = DF[token] if token in vocab else 0\n",
    "        inverse_document_frequency = m.log((len(all_docs) + 1) / (document_frequency + 1))\n",
    "\n",
    "        try:\n",
    "            index = vocab.index(token)\n",
    "            vector[index] = term_frequency * inverse_document_frequency\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return vector\n",
    "\n",
    "#Utility function to calculate cosine similarity between 2 vectors\n",
    "def cosine_similarity(x, y):\n",
    "    \"\"\"Calculate cosine similarity between 2 vectors (same dimension).\n",
    "    Arguments:\n",
    "        x {numpy.ndarray} -- vector 1\n",
    "        y {numpy.ndarray} -- vector 2\n",
    "    \n",
    "    Returns:\n",
    "        numpy.float64 -- cosine similarity between vector 1 & vector 2\n",
    "    \"\"\"\n",
    "    if not (np.issubdtype(x.dtype, np.number) and np.issubdtype(y.dtype, np.number)):\n",
    "        raise ValueError(\"Input vectors must have numeric data types.\")\n",
    "    if x.shape != y.shape:\n",
    "        raise ValueError(\"Input vectors must have the same dimensions.\")\n",
    "    \n",
    "    if np.all(x == 0) or np.all(y == 0):\n",
    "        return 0\n",
    "    return np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next snippet there's the function to generate the ranked retrived list given the query and the number $k$ of retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to rank documents based on cosine similarity with query\n",
    "\n",
    "def ranking(k, query, TF_IDF):\n",
    "    \"\"\"Determines a ranked list of top k documents in descending order of their\n",
    "    cosine similarity with the query\n",
    "    \n",
    "    Arguments:\n",
    "        k {integer} -- top k documents to retrieve (if k=0, retrieve all documents)\n",
    "        query {string} -- query whose cosine similarity has to be computed with the corpus\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray -- list of indexes of top k cosine similarities between query and corpus of documents\n",
    "    \"\"\"\n",
    "    if k < 0:\n",
    "        print(\"Please enter a value greater than or equal to 0\")\n",
    "        return\n",
    "    if(len(query) == 0):\n",
    "        print(\"Please enter a valid query\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    document_cosine_similarities = []\n",
    "    \n",
    "    for doc_id, document_vector in enumerate(TF_IDF):\n",
    "            \n",
    "        if type(query) is str:\n",
    "            # if query is a string of tokens convert it to a vector using tf-idf\n",
    "            query_vector = generate_vector(query.split())\n",
    "        \n",
    "        if type(query) is np.ndarray:\n",
    "            # if query is already a vector, use it as it is\n",
    "            query_vector = query\n",
    "        \n",
    "        if(len(query_vector) != len(vocab)):\n",
    "            print(\"Error on generating vector for query\")\n",
    "            return\n",
    "        document_cosine_similarities.append(cosine_similarity(query_vector, document_vector))\n",
    "        \n",
    "    if k == 0:\n",
    "        # k=0 to retrieve all documents in descending order\n",
    "        output_indices = np.array(document_cosine_similarities).argsort()[::-1]\n",
    "        \n",
    "    else:\n",
    "        # to retrieve the top k documents in descending order    \n",
    "        output_indices = np.array(document_cosine_similarities).argsort()[-k:][::-1]\n",
    "    \n",
    "    return output_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Model Evaluation: precision and recall.\n",
    "\n",
    "For the evaluation we consider the $\\texttt{relevance.txt}$ file which is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Query_ID  Relevance\n",
      "0         0        155\n",
      "1         1        665\n",
      "2         1        666\n",
      "3         1       1257\n",
      "4         1       1393\n",
      "Relevant docs for each query:  [[155], [665, 666, 1257, 1393, 667, 669, 1203, 1390, 1394, 1299, 36, 558, 629, 1106, 1212], [23, 100, 665, 666, 92, 1257, 1392, 558, 629, 661, 1103, 1106, 1203, 1212, 1299], [1390, 665, 666, 1257, 1077, 1079, 1080, 1393, 1394, 1213, 1197, 1203, 1299, 558, 629, 661, 1106, 1212], [1382, 1384, 154, 240, 1381, 1369, 1385, 110, 1383, 149, 291, 457, 478, 976, 375, 458, 1364, 61, 1365], [154, 1382, 1384, 1381, 61, 291, 240, 1369, 1383, 457, 458, 460, 1385, 1364, 1365, 110, 149, 478], [399, 418, 1386, 411, 1391, 1397, 1396, 1399, 1398], [399, 1386, 1391, 1397], [655, 1312, 1316, 1315, 1317, 1318, 1156, 1273], [1378, 1304, 1303, 39, 292, 1308, 160, 420, 1376, 1377, 1380, 224, 1379, 447, 448, 1123, 1279, 432, 922, 923, 1061, 1073, 1074, 1212]]\n"
     ]
    }
   ],
   "source": [
    "#Get the relevance of each document for each query\n",
    "# NB: queries and docs numbered from 1.\n",
    "column_names = ['Query_ID', 'Relevance']\n",
    "relevance_df = pd.read_csv(f\"{path}/relevance.txt\", delim_whitespace=True, names=column_names, header=None)\n",
    "relevance_df['Relevance']=relevance_df['Relevance'].astype(int)\n",
    "relevance_df=relevance_df - 1 # to make the indexing of queries and docs start from 0\n",
    "print(relevance_df.head())\n",
    "\n",
    "query_file = open(path+'/preprocessed_queries.txt', 'r')\n",
    "queries = query_file.readlines()\n",
    "\n",
    "\n",
    "query_relevance_lists = []\n",
    "for query_id in range(len(queries)):\n",
    "    relevant_documents = relevance_df[relevance_df['Query_ID'] == query_id]['Relevance'].to_list()\n",
    "    query_relevance_lists.append(relevant_documents)\n",
    "print(f'Relevant docs for each query: ', query_relevance_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following snippet we calculate the recall for a given k (the number of retrieved documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall values:  [0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.11, 0.0, 0.0, 0.04]\n",
      "Mean recall:  0.026\n"
     ]
    }
   ],
   "source": [
    "# Calculating recall= number of relevant documents retrieved/total number of relevant documents\n",
    "# Get the relevance of each document for each query\n",
    "def calculate_recall_at_k(k=50):\n",
    "    \"\"\"\n",
    "    Calculate recall values for each query given the number of top documents to be retrieved (k).\n",
    "\n",
    "    Args:\n",
    "        k (int): Number of top documents to be retrieved.\n",
    "\n",
    "    Returns:\n",
    "        list: List of recall values for each query.\n",
    "    \"\"\"\n",
    "    recall_values = []\n",
    "\n",
    "    for query_id in range(len(queries)):\n",
    "        relevant_documents = query_relevance_lists[query_id] # Solution\n",
    "        top_k_documents = ranking(k, queries[query_id], TF_IDF)\n",
    "        #with TF-IDF= np.random.rand(len(all_docs), len(vocab)) results are worse (avg is a quarter)\n",
    "\n",
    "        # Number of relevant documents retrieved\n",
    "        intersect_count = len([value for value in top_k_documents if value in relevant_documents])\n",
    "\n",
    "        # Total number of relevant documents\n",
    "        total_relevant_documents = len(relevant_documents)\n",
    "\n",
    "        # Calculate recall for the current query\n",
    "        recall = intersect_count / total_relevant_documents if total_relevant_documents > 0 else 0\n",
    "        recall_values.append(recall)\n",
    "\n",
    "    return recall_values\n",
    "\n",
    "recall_results = calculate_recall_at_k(50)\n",
    "#rounding off to 2 decimal places\n",
    "recall_results = [round(num, 2) for num in recall_results]\n",
    "print(f'Recall values: ', recall_results)\n",
    "print(f'Mean recall: ', round(np.mean(recall_results), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following snippet we calculate the precision for a given k (the number of retrieved documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.02, 0.0, 0.0, 0.02]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating precision= number of relevant documents retrieved/total number of documents retrieved\n",
    "def calculate_precision_at_k(k=50):\n",
    "    \"\"\"To generate list of precision values for each query for given value of k\n",
    "    \n",
    "    Arguments:\n",
    "        k {[type]} -- number of top documents to be retrieved\n",
    "    \n",
    "    Returns:\n",
    "        list -- list of precision values for each query\n",
    "    \"\"\"\n",
    "    precision_values = []\n",
    "    for query_id in range(len(queries)):\n",
    "        relevant_documents = query_relevance_lists[query_id] # Solution\n",
    "        \n",
    "        top_k_documents = ranking(k, queries[query_id], TF_IDF)\n",
    "\n",
    "        # Number of relevant documents retrieved\n",
    "        intersect_count = len([value for value in top_k_documents if value in relevant_documents])\n",
    "\n",
    "        # Calculate recall for the current query\n",
    "        precision = intersect_count / k\n",
    "        precision_values.append(precision)\n",
    "\n",
    "    return precision_values\n",
    "#rounding off to 2 decimal places\n",
    "precision_results = [round(num, 2) for num in calculate_precision_at_k(50)]\n",
    "calculate_precision_at_k(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform the analysis of recall and precision for different values of retrieved documents. Here I save the values on the text file: $\\texttt{accuracy\\_without\\_feedback}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing precision and recall values to file\n",
    "list_of_k = [20, 50, 100, 200]\n",
    "# If file already exists, delete it\n",
    "if not os.path.exists('accuracy_without_feedbacks'):\n",
    "    with open('accuracy_without_feedbacks', 'w') as file:\n",
    "        for k in list_of_k:\n",
    "            p = calculate_precision_at_k(k)\n",
    "            r = calculate_recall_at_k(k)\n",
    "            \n",
    "            file.write(f\"Top {k} documents in the rank list\\n\")\n",
    "            \n",
    "            for i in range(len(queries)):\n",
    "                approximated_recall = round(r[i], 2)\n",
    "                file.write(\"Query: {0} \\t Pr: {1} \\t Re: {2}\\n\".format(i+1, p[i], approximated_recall))\n",
    "            \n",
    "            avg_precision = round(np.mean(p), 2)\n",
    "            avg_recall = round(np.mean(r), 2)\n",
    "            \n",
    "            file.write(\"Avg Precision: {0}\\n\".format(avg_precision))\n",
    "            file.write(\"Avg Recall: {0}\\n\\n\".format(avg_recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Feedback\n",
    "\n",
    "In the following snippets I will implement the Rocchio algorithm to improve the information retrieval systems. The feedback information must be stored in the $\\texttt{relevant\\_docs}$ variable (list of relevant documents for each query). \n",
    "\n",
    "To consider custom relevance feedback just modify the $\\texttt{custom\\_relevance.txt}$ file (with same structure of $\\texttt{relevance.txt}$ file: list of {query_id, doc_id} pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relevance of each document for each query\n",
    "relevant_docs= query_relevance_lists #here using the relevance list given by the dataset as default\n",
    "\n",
    "#if existing,considering the custom relevance list (dafult is the one given by the dataset, a.k.a. relevance.txt)\n",
    "if os.path.exists(path + '/custom_relevance.txt'):\n",
    "    with open(path+ '/custom_relevance.txt', 'r') as file:\n",
    "        custom_relevance_df = pd.read_csv(f\"{path}/custom_relevance.txt\", delim_whitespace=True, names=column_names, header=None)\n",
    "        custom_relevance_df['Relevance']=relevance_df['Relevance'].astype(int)\n",
    "        custom_relevance_df=relevance_df - 1 \n",
    "    query_relevance_lists = []\n",
    "    for query_id in range(len(queries)):\n",
    "        relevant_documents = relevance_df[relevance_df['Query_ID'] == query_id]['Relevance'].to_list()\n",
    "        query_relevance_lists.append(relevant_documents)\n",
    "else:\n",
    "    print(\"No custom relevance file found. Using default relevance list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rocchio Algorithm\n",
    "# Intializing alpha, beta, gamma values\n",
    "alpha = 1\n",
    "beta = 3 #0.75\n",
    "gamma = 0.15\n",
    "\n",
    "# Calculate centroid of relevant/irrelevant documents\n",
    "def calculate_centroid(docs_indexes):\n",
    "    \"\"\"\n",
    "    Calculate centroid of relevant/irrelevant documents.\n",
    "\n",
    "    Args:\n",
    "        docs_indexes (list): List of relevant documents.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Centroid vector of relevant documents.\n",
    "    \"\"\"\n",
    "    centroid = np.zeros(len(vocab))\n",
    "\n",
    "\n",
    "    for document_id in docs_indexes:\n",
    "        if(document_id >= len(all_docs)):\n",
    "            print(\"Invalid document index\")\n",
    "            return\n",
    "        if(len(TF_IDF[document_id,:]) != len(vocab)):\n",
    "            print(\"Error on generating vector for document\")\n",
    "            return\n",
    "        centroid += np.array(TF_IDF[document_id,:]).flatten()\n",
    "\n",
    "    return centroid / len(docs_indexes)\n",
    "\n",
    "\n",
    "def calculate_new_query_vector(query_id, relevant_docs_index ,alpha=1, beta=0.75, gamma=0.15):\n",
    "    \"\"\"\n",
    "    Calculate new query vector based on Rocchio algorithm.\n",
    "\n",
    "    Args:\n",
    "        query_id (int): Query ID.\n",
    "        relevant_documents_index (list): List of indexes relevant documents.\n",
    "        alpha (float): Weight for the original query vector.\n",
    "        beta (float): Weight for the centroid of relevant documents.\n",
    "        gamma (float): Weight for the centroid of non-relevant documents.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: New query vector.\n",
    "    \"\"\"\n",
    "    original_query_vector = generate_vector(queries[query_id].split())\n",
    "    # Get the list of relevant and non-relevant document IDs for the current query\n",
    "    non_relevant_docs_index= [doc_id for doc_id in range(len(all_docs)) if doc_id not in relevant_docs_index]\n",
    "    #print(non_relevant_document_ids)\n",
    "    # Calculate Rocchio algorithm components\n",
    "    relevant_centroid = calculate_centroid(relevant_docs_index)\n",
    "    non_relevant_centroid = calculate_centroid(non_relevant_docs_index)\n",
    "\n",
    "    # Calculate new query vector using Rocchio algorithm\n",
    "    new_query_vector = alpha * original_query_vector + beta * relevant_centroid - gamma * non_relevant_centroid\n",
    "\n",
    "    return new_query_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the performance in the same way as before. Obviously by considering the solution as feedback and high value of $\\beta$ (which represents the weight of the centroid given by the feedback) we obtain great results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall values with relevant feedback giveng by the solution: [1.0, 0.13, 0.0, 0.33, 0.16, 0.39, 0.11, 1.0, 1.0, 0.12]\n",
      "Precision values with relevant feedback giveng by the solution: [0.033, 0.067, 0.0, 0.2, 0.1, 0.233, 0.033, 0.133, 0.267, 0.1]\n"
     ]
    }
   ],
   "source": [
    "# Calculating recall= number of relevant documents retrieved/total number of relevant documents\n",
    "\n",
    "# Get the relevance of each document for each query\n",
    "def calculate_recall_at_k_with_feedback(k=50):\n",
    "    \"\"\"\n",
    "    Calculate recall values for each query given the number of top documents to be retrieved (k).\n",
    "\n",
    "    Args:\n",
    "        k (int): Number of top documents to be retrieved.\n",
    "\n",
    "    Returns:\n",
    "        list: List of recall values for each query.\n",
    "    \"\"\"\n",
    "    recall_values = []\n",
    "\n",
    "    for query_id in range(len(queries)):\n",
    "        relevant_documents = query_relevance_lists[query_id]\n",
    "        new_query = calculate_new_query_vector(query_id, relevant_documents, alpha, beta, gamma)\n",
    "        top_k_documents = ranking(k, new_query, TF_IDF)\n",
    "        # Number of relevant documents retrieved\n",
    "        intersect_count = len([value for value in top_k_documents if value in relevant_documents])\n",
    "\n",
    "        # Total number of relevant documents\n",
    "        total_relevant_documents = len(relevant_documents)\n",
    "\n",
    "        # Calculate recall for the current query\n",
    "        recall = intersect_count / total_relevant_documents\n",
    "        recall_values.append(recall)\n",
    "\n",
    "    return recall_values\n",
    "\n",
    "#rounding off to 2 decimal places\n",
    "recall_results = [round(num, 2) for num in calculate_recall_at_k_with_feedback(30)]\n",
    "print(f'Recall values with relevant feedback giveng by the solution:', recall_results)\n",
    "\n",
    "#Calculating precision= number of relevant documents retrieved/total number of documents retrieved\n",
    "def calculate_precision_at_k_with_feedback(k=50):\n",
    "    \"\"\"To generate list of precision values for each query for given value of k\n",
    "    \n",
    "    Arguments:\n",
    "        k {[type]} -- number of top documents to be retrieved\n",
    "    \n",
    "    Returns:\n",
    "        list -- list of precision values for each query\n",
    "    \"\"\"\n",
    "    precision_values = []\n",
    "    for query_id in range(len(queries)):\n",
    "        relevant_documents = query_relevance_lists[query_id]\n",
    "        new_query = calculate_new_query_vector(query_id, relevant_documents, alpha, beta, gamma)\n",
    "        top_k_documents = ranking(k, new_query, TF_IDF)\n",
    "        # Number of relevant documents retrieved\n",
    "        intersect_count = len([value for value in top_k_documents if value in relevant_documents])\n",
    "        # Calculate recall for the current query\n",
    "        precision = intersect_count / k\n",
    "        precision_values.append(precision)\n",
    "\n",
    "    return precision_values\n",
    "#rounding off to 2 decimal places\n",
    "precision_results = [round(num, 3) for num in calculate_precision_at_k_with_feedback(30)]\n",
    "print(f'Precision values with relevant feedback giveng by the solution:', precision_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Psuedo Relevance Feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following snippet I implement the pseudo relevance feedback: the idea is to assume the top $k$ document retrieved by our vector model as feedback information and then update the query vector with this information as done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall values with pseudo relevant feedback: [0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.11, 0.0, 0.0, 0.04]\n",
      "Mean recall with pseudo relevant feedback: 0.026\n"
     ]
    }
   ],
   "source": [
    "# Calculating recall= number of relevant documents retrieved/total number of relevant documents\n",
    "\n",
    "# Get the relevance of each document for each query\n",
    "def calculate_recall_at_k_with_pseudo_feedback(k=50):\n",
    "    \"\"\"\n",
    "    Calculate recall values for each query given the number of top documents to be retrieved (k).\n",
    "\n",
    "    Args:\n",
    "        k (int): Number of top documents to be retrieved.\n",
    "\n",
    "    Returns:\n",
    "        list: List of recall values for each query.\n",
    "    \"\"\"\n",
    "    recall_values = []\n",
    "    for query_id in range(len(queries)):\n",
    "        relevant_documents = query_relevance_lists[query_id] # Solution\n",
    "        \n",
    "        top_k_documents = ranking(k, queries[query_id], TF_IDF)\n",
    "    \n",
    "        new_query = calculate_new_query_vector(query_id, top_k_documents, alpha, beta, gamma)\n",
    "\n",
    "        top_k_documents = ranking(k, new_query, TF_IDF)\n",
    "\n",
    "        # Number of relevant documents retrieved\n",
    "        intersect_count = len([value for value in top_k_documents if value in relevant_documents])\n",
    "\n",
    "        # Total number of relevant documents\n",
    "        total_relevant_documents = len(relevant_documents)\n",
    "\n",
    "        # Calculate recall for the current query\n",
    "        recall = intersect_count / total_relevant_documents\n",
    "        recall_values.append(recall)\n",
    "\n",
    "    return recall_values\n",
    "\n",
    "#rounding off to 2 decimal places\n",
    "recall_results = [round(num, 2) for num in calculate_recall_at_k_with_pseudo_feedback(50)]\n",
    "print(f'Recall values with pseudo relevant feedback:', recall_results)\n",
    "print(f'Mean recall with pseudo relevant feedback:', round(np.mean(recall_results), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision values with relevant feedback giveng by the solution: [0.0, 0.0, 0.0, 0.0, 0.033, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Mean precision with pseudo relevant feedback: 0.003\n"
     ]
    }
   ],
   "source": [
    "# Calculating recall= number of relevant documents retrieved/total number of relevant documents\n",
    "\n",
    "# Get the relevance of each document for each query\n",
    "def calculate_precision_at_k_with_pseudo_feedback(k=50):\n",
    "    \"\"\"\n",
    "    Calculate precision values for each query given the number of top documents to be retrieved (k).\n",
    "\n",
    "    Args:\n",
    "        k (int): Number of top documents to be retrieved.\n",
    "\n",
    "    Returns:\n",
    "        list: List of recall values for each query.\n",
    "    \"\"\"\n",
    "    precision_values = []\n",
    "    for query_id in range(len(queries)):\n",
    "        relevant_documents = query_relevance_lists[query_id] # Solution\n",
    "        \n",
    "        top_k_documents = ranking(k, queries[query_id], TF_IDF)\n",
    "    \n",
    "        new_query = calculate_new_query_vector(query_id, top_k_documents, alpha, beta, gamma)\n",
    "\n",
    "        top_k_documents = ranking(k, new_query, TF_IDF)\n",
    "\n",
    "        # Number of relevant documents retrieved\n",
    "        intersect_count = len([value for value in top_k_documents if value in relevant_documents])\n",
    "\n",
    "        # Total number of relevant documents\n",
    "        total_relevant_documents = len(relevant_documents)\n",
    "        precision = intersect_count / k\n",
    "        precision_values.append(precision)\n",
    "\n",
    "    return precision_values\n",
    "\n",
    "#rounding off to 2 decimal places\n",
    "precision_results = [round(num, 3) for num in calculate_precision_at_k_with_pseudo_feedback(30)]\n",
    "print(f'Precision values with relevant feedback giveng by the solution:', precision_results)\n",
    "print(f'Mean precision with pseudo relevant feedback:', round(np.mean(precision_results), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
